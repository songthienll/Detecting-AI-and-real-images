{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf4dffc",
   "metadata": {
    "papermill": {
     "duration": 0.005242,
     "end_time": "2025-05-28T01:32:25.524187",
     "exception": false,
     "start_time": "2025-05-28T01:32:25.518945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### IMPORT & FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4852dc74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:32:25.533425Z",
     "iopub.status.busy": "2025-05-28T01:32:25.533028Z",
     "iopub.status.idle": "2025-05-28T01:33:14.141010Z",
     "shell.execute_reply": "2025-05-28T01:33:14.140214Z"
    },
    "papermill": {
     "duration": 48.614672,
     "end_time": "2025-05-28T01:33:14.142892",
     "exception": false,
     "start_time": "2025-05-28T01:32:25.528220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 01:32:51.065940: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748395971.569562      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748395971.698503      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tempfile import TemporaryDirectory\n",
    "from PIL import ImageFile\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb57d797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:14.154531Z",
     "iopub.status.busy": "2025-05-28T01:33:14.153734Z",
     "iopub.status.idle": "2025-05-28T01:33:14.158186Z",
     "shell.execute_reply": "2025-05-28T01:33:14.157475Z"
    },
    "papermill": {
     "duration": 0.010251,
     "end_time": "2025-05-28T01:33:14.159465",
     "exception": false,
     "start_time": "2025-05-28T01:33:14.149214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix truncated images - For this dataset\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d75ebc",
   "metadata": {
    "papermill": {
     "duration": 0.003273,
     "end_time": "2025-05-28T01:33:14.166231",
     "exception": false,
     "start_time": "2025-05-28T01:33:14.162958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d49c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:14.174138Z",
     "iopub.status.busy": "2025-05-28T01:33:14.173630Z",
     "iopub.status.idle": "2025-05-28T01:33:14.179727Z",
     "shell.execute_reply": "2025-05-28T01:33:14.178872Z"
    },
    "papermill": {
     "duration": 0.011429,
     "end_time": "2025-05-28T01:33:14.180954",
     "exception": false,
     "start_time": "2025-05-28T01:33:14.169525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform data\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4502, 0.4242, 0.3926], [0.2224, 0.2084, 0.2035])\n",
    "    ]), \n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4502, 0.4242, 0.3926], [0.2224, 0.2084, 0.2035])\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4502, 0.4242, 0.3926], [0.2224, 0.2084, 0.2035])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf4c349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:14.188838Z",
     "iopub.status.busy": "2025-05-28T01:33:14.188385Z",
     "iopub.status.idle": "2025-05-28T01:33:14.192289Z",
     "shell.execute_reply": "2025-05-28T01:33:14.191480Z"
    },
    "papermill": {
     "duration": 0.009125,
     "end_time": "2025-05-28T01:33:14.193551",
     "exception": false,
     "start_time": "2025-05-28T01:33:14.184426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Declare directories\n",
    "data_dir = \"/kaggle/input/ai-and-real-images\"\n",
    "folders = [\"train\", \"val\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2e2ab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:14.200858Z",
     "iopub.status.busy": "2025-05-28T01:33:14.200450Z",
     "iopub.status.idle": "2025-05-28T01:33:39.843183Z",
     "shell.execute_reply": "2025-05-28T01:33:39.842029Z"
    },
    "papermill": {
     "duration": 25.648808,
     "end_time": "2025-05-28T01:33:39.845446",
     "exception": false,
     "start_time": "2025-05-28T01:33:14.196638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load image datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in folders}\n",
    "\n",
    "# Loader for mini batch\n",
    "loader = {x: DataLoader(image_datasets[x], batch_size = 32, shuffle = True, num_workers = 4) for x in folders}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313dcadc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:39.855176Z",
     "iopub.status.busy": "2025-05-28T01:33:39.854293Z",
     "iopub.status.idle": "2025-05-28T01:33:39.859113Z",
     "shell.execute_reply": "2025-05-28T01:33:39.858359Z"
    },
    "papermill": {
     "duration": 0.010851,
     "end_time": "2025-05-28T01:33:39.860522",
     "exception": false,
     "start_time": "2025-05-28T01:33:39.849671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset sizes and class names\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in folders}\n",
    "class_names = image_datasets[\"train\"].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05cdc92",
   "metadata": {
    "papermill": {
     "duration": 0.003214,
     "end_time": "2025-05-28T01:33:39.867504",
     "exception": false,
     "start_time": "2025-05-28T01:33:39.864290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CONFIGURE & TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7260bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:39.875583Z",
     "iopub.status.busy": "2025-05-28T01:33:39.875232Z",
     "iopub.status.idle": "2025-05-28T01:33:40.060717Z",
     "shell.execute_reply": "2025-05-28T01:33:40.059798Z"
    },
    "papermill": {
     "duration": 0.191499,
     "end_time": "2025-05-28T01:33:40.062408",
     "exception": false,
     "start_time": "2025-05-28T01:33:39.870909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get torch device\n",
    "device = torch.device(\"cuda\" if torch.accelerator.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c71263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:40.071092Z",
     "iopub.status.busy": "2025-05-28T01:33:40.070733Z",
     "iopub.status.idle": "2025-05-28T01:33:40.075308Z",
     "shell.execute_reply": "2025-05-28T01:33:40.074475Z"
    },
    "papermill": {
     "duration": 0.010281,
     "end_time": "2025-05-28T01:33:40.076670",
     "exception": false,
     "start_time": "2025-05-28T01:33:40.066389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make directory for saving model and stats\n",
    "os.mkdir(\"/kaggle/working/runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c59320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:40.084647Z",
     "iopub.status.busy": "2025-05-28T01:33:40.084383Z",
     "iopub.status.idle": "2025-05-28T01:33:40.090467Z",
     "shell.execute_reply": "2025-05-28T01:33:40.089813Z"
    },
    "papermill": {
     "duration": 0.011485,
     "end_time": "2025-05-28T01:33:40.091760",
     "exception": false,
     "start_time": "2025-05-28T01:33:40.080275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use tensorboard to store stats and make a directory to save\n",
    "os.mkdir(\"/kaggle/working/runs/tensorboard\")\n",
    "writer = SummaryWriter(log_dir='/kaggle/working/runs/tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83299c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:40.100384Z",
     "iopub.status.busy": "2025-05-28T01:33:40.099626Z",
     "iopub.status.idle": "2025-05-28T01:33:40.110129Z",
     "shell.execute_reply": "2025-05-28T01:33:40.109244Z"
    },
    "papermill": {
     "duration": 0.016148,
     "end_time": "2025-05-28T01:33:40.111498",
     "exception": false,
     "start_time": "2025-05-28T01:33:40.095350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch: Train phase and Val phase\n",
    "            for phase in [\"train\", \"test\"]:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data\n",
    "                for inputs, labels in loader[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # Zero gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward\n",
    "                    with torch.set_grad_enabled(phase == 'train'): # grad_enable: True (Train), False (Val)\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Backward + Optimize\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Stats\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "                \n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                writer.add_scalar(f'{phase}/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar(f'{phase}/Accuracy', epoch_acc, epoch)\n",
    "\n",
    "                # Deep copy the model\n",
    "                if phase == 'test' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13cd061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:40.119426Z",
     "iopub.status.busy": "2025-05-28T01:33:40.119157Z",
     "iopub.status.idle": "2025-05-28T01:33:42.454947Z",
     "shell.execute_reply": "2025-05-28T01:33:42.454033Z"
    },
    "papermill": {
     "duration": 2.341645,
     "end_time": "2025-05-28T01:33:42.456825",
     "exception": false,
     "start_time": "2025-05-28T01:33:40.115180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
      "100%|██████████| 108M/108M [00:00<00:00, 144MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): SwinTransformer(\n",
       "    (features): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): Permute()\n",
       "        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): PatchMerging(\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): PatchMerging(\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): PatchMerging(\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (permute): Permute()\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (head): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the pretrain model\n",
    "model_ft = models.swin_t(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Change the head layer\n",
    "model_ft.head = torch.nn.Linear(model_ft.head.in_features, len(class_names))\n",
    "\n",
    "# Check if it multigpu or not to using DataParallel (MultiGPU training)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_ft = torch.nn.DataParallel(model_ft)\n",
    "model_ft.to(device)\n",
    "\n",
    "### Output is a architecture of model after changing the out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f96645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:42.467890Z",
     "iopub.status.busy": "2025-05-28T01:33:42.467538Z",
     "iopub.status.idle": "2025-05-28T01:33:42.473707Z",
     "shell.execute_reply": "2025-05-28T01:33:42.473019Z"
    },
    "papermill": {
     "duration": 0.013344,
     "end_time": "2025-05-28T01:33:42.475129",
     "exception": false,
     "start_time": "2025-05-28T01:33:42.461785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross Entropy\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer - lr e-3, momentum 0.9\n",
    "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Step LR - step_size 1, gamma 0.9\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "608679b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:33:42.487520Z",
     "iopub.status.busy": "2025-05-28T01:33:42.486502Z",
     "iopub.status.idle": "2025-05-28T07:22:28.670455Z",
     "shell.execute_reply": "2025-05-28T07:22:28.669306Z"
    },
    "papermill": {
     "duration": 20926.192583,
     "end_time": "2025-05-28T07:22:28.672489",
     "exception": false,
     "start_time": "2025-05-28T01:33:42.479906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4366 Acc: 0.7946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.3394 Acc: 0.8500\n",
      "\n",
      "Epoch 2/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3338 Acc: 0.8543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2596 Acc: 0.8925\n",
      "\n",
      "Epoch 3/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2891 Acc: 0.8761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2393 Acc: 0.9055\n",
      "\n",
      "Epoch 4/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2613 Acc: 0.8929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2400 Acc: 0.9055\n",
      "\n",
      "Epoch 5/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2386 Acc: 0.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2302 Acc: 0.9050\n",
      "\n",
      "Epoch 6/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2127 Acc: 0.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2226 Acc: 0.9098\n",
      "\n",
      "Epoch 7/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1986 Acc: 0.9190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1988 Acc: 0.9248\n",
      "\n",
      "Epoch 8/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1883 Acc: 0.9228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2034 Acc: 0.9198\n",
      "\n",
      "Epoch 9/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1706 Acc: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1748 Acc: 0.9323\n",
      "\n",
      "Epoch 10/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1601 Acc: 0.9361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1809 Acc: 0.9303\n",
      "\n",
      "Epoch 11/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1493 Acc: 0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1780 Acc: 0.9285\n",
      "\n",
      "Epoch 12/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1428 Acc: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1653 Acc: 0.9370\n",
      "\n",
      "Epoch 13/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1315 Acc: 0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2191 Acc: 0.9198\n",
      "\n",
      "Epoch 14/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1330 Acc: 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1606 Acc: 0.9415\n",
      "\n",
      "Epoch 15/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1239 Acc: 0.9528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1531 Acc: 0.9460\n",
      "\n",
      "Epoch 16/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1190 Acc: 0.9536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1569 Acc: 0.9447\n",
      "\n",
      "Epoch 17/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1195 Acc: 0.9555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1609 Acc: 0.9415\n",
      "\n",
      "Epoch 18/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1122 Acc: 0.9558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1579 Acc: 0.9440\n",
      "\n",
      "Epoch 19/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1070 Acc: 0.9572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1573 Acc: 0.9433\n",
      "\n",
      "Epoch 20/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1033 Acc: 0.9594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1528 Acc: 0.9483\n",
      "\n",
      "Epoch 21/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1023 Acc: 0.9612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1496 Acc: 0.9460\n",
      "\n",
      "Epoch 22/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0987 Acc: 0.9628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1523 Acc: 0.9457\n",
      "\n",
      "Epoch 23/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0962 Acc: 0.9656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1527 Acc: 0.9455\n",
      "\n",
      "Epoch 24/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0902 Acc: 0.9657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1512 Acc: 0.9495\n",
      "\n",
      "Epoch 25/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0935 Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1516 Acc: 0.9497\n",
      "\n",
      "Training complete in 348m 46s\n",
      "Best val Acc: 0.949750\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL\n",
    "model_conv = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d1698",
   "metadata": {
    "papermill": {
     "duration": 0.010729,
     "end_time": "2025-05-28T07:22:28.695794",
     "exception": false,
     "start_time": "2025-05-28T07:22:28.685065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SAVE & PLOT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fe2b455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:22:28.720063Z",
     "iopub.status.busy": "2025-05-28T07:22:28.719164Z",
     "iopub.status.idle": "2025-05-28T07:22:28.952997Z",
     "shell.execute_reply": "2025-05-28T07:22:28.952186Z"
    },
    "papermill": {
     "duration": 0.24805,
     "end_time": "2025-05-28T07:22:28.954772",
     "exception": false,
     "start_time": "2025-05-28T07:22:28.706722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "os.mkdir(\"/kaggle/working/runs/model\")\n",
    "torch.save(model_conv.state_dict(), \"/kaggle/working/runs/model/best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a08ed12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:22:28.981953Z",
     "iopub.status.busy": "2025-05-28T07:22:28.981191Z",
     "iopub.status.idle": "2025-05-28T07:24:10.565298Z",
     "shell.execute_reply": "2025-05-28T07:24:10.564070Z"
    },
    "papermill": {
     "duration": 101.614452,
     "end_time": "2025-05-28T07:24:10.582154",
     "exception": false,
     "start_time": "2025-05-28T07:22:28.967702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run model with test set\n",
    "model_conv.eval() # Eval mode\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for inputs, labels in loader[\"val\"]:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Forward - Grad disable\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model_conv(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    y_pred.append(preds)\n",
    "    y_true.append(labels)\n",
    "\n",
    "# Concatenates all the mini-batch\n",
    "y_pred = torch.cat(tuple(y_pred))\n",
    "y_true = torch.cat(tuple(y_true))\n",
    "\n",
    "# Confusion matrix\n",
    "cfs_mtx = confusion_matrix(y_true.cpu(), y_pred.cpu())\n",
    "display = ConfusionMatrixDisplay(cfs_mtx)\n",
    "\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d52ca343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T07:24:10.610395Z",
     "iopub.status.busy": "2025-05-28T07:24:10.609330Z",
     "iopub.status.idle": "2025-05-28T07:24:10.960857Z",
     "shell.execute_reply": "2025-05-28T07:24:10.959444Z"
    },
    "papermill": {
     "duration": 0.367248,
     "end_time": "2025-05-28T07:24:10.962686",
     "exception": true,
     "start_time": "2025-05-28T07:24:10.595438",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConfusionMatrixDisplay' object has no attribute '_figure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/790637882.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"confusion matrix - test set\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_figure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConfusionMatrixDisplay' object has no attribute '_figure'"
     ]
    }
   ],
   "source": [
    "writer.add_figure(\"confusion matrix - test set\", display._figure)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7475477,
     "sourceId": 11893043,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21116.548455,
   "end_time": "2025-05-28T07:24:14.236480",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T01:32:17.688025",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
