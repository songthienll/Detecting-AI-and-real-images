{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b6db32",
   "metadata": {
    "papermill": {
     "duration": 0.003745,
     "end_time": "2025-05-27T10:32:02.071644",
     "exception": false,
     "start_time": "2025-05-27T10:32:02.067899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### IMPORT & FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f778daa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:32:02.078712Z",
     "iopub.status.busy": "2025-05-27T10:32:02.078487Z",
     "iopub.status.idle": "2025-05-27T10:32:28.670259Z",
     "shell.execute_reply": "2025-05-27T10:32:28.669581Z"
    },
    "papermill": {
     "duration": 26.596973,
     "end_time": "2025-05-27T10:32:28.671809",
     "exception": false,
     "start_time": "2025-05-27T10:32:02.074836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 10:32:15.255542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748341935.473713      18 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748341935.535519      18 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tempfile import TemporaryDirectory\n",
    "from PIL import ImageFile\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5183c134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:32:28.679681Z",
     "iopub.status.busy": "2025-05-27T10:32:28.679228Z",
     "iopub.status.idle": "2025-05-27T10:32:28.682707Z",
     "shell.execute_reply": "2025-05-27T10:32:28.682233Z"
    },
    "papermill": {
     "duration": 0.008431,
     "end_time": "2025-05-27T10:32:28.683719",
     "exception": false,
     "start_time": "2025-05-27T10:32:28.675288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix truncated images - For this dataset\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86cc524",
   "metadata": {
    "papermill": {
     "duration": 0.003007,
     "end_time": "2025-05-27T10:32:28.690008",
     "exception": false,
     "start_time": "2025-05-27T10:32:28.687001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d76405b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:32:28.696750Z",
     "iopub.status.busy": "2025-05-27T10:32:28.696520Z",
     "iopub.status.idle": "2025-05-27T10:32:28.701653Z",
     "shell.execute_reply": "2025-05-27T10:32:28.701091Z"
    },
    "papermill": {
     "duration": 0.009776,
     "end_time": "2025-05-27T10:32:28.702709",
     "exception": false,
     "start_time": "2025-05-27T10:32:28.692933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform data\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4502, 0.4242, 0.3926], [0.2224, 0.2084, 0.2035])\n",
    "    ]), \n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4502, 0.4242, 0.3926], [0.2224, 0.2084, 0.2035])\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4502, 0.4242, 0.3926], [0.2224, 0.2084, 0.2035])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08132a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:32:28.709495Z",
     "iopub.status.busy": "2025-05-27T10:32:28.709304Z",
     "iopub.status.idle": "2025-05-27T10:32:28.712452Z",
     "shell.execute_reply": "2025-05-27T10:32:28.711940Z"
    },
    "papermill": {
     "duration": 0.007828,
     "end_time": "2025-05-27T10:32:28.713539",
     "exception": false,
     "start_time": "2025-05-27T10:32:28.705711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Declare directories\n",
    "data_dir = \"/kaggle/input/ai-and-real-images\"\n",
    "folders = [\"train\", \"val\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea0310e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:32:28.720802Z",
     "iopub.status.busy": "2025-05-27T10:32:28.720193Z",
     "iopub.status.idle": "2025-05-27T10:33:04.733513Z",
     "shell.execute_reply": "2025-05-27T10:33:04.732829Z"
    },
    "papermill": {
     "duration": 36.018312,
     "end_time": "2025-05-27T10:33:04.734959",
     "exception": false,
     "start_time": "2025-05-27T10:32:28.716647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load image datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in folders}\n",
    "\n",
    "# Loader for mini batch\n",
    "loader = {x: DataLoader(image_datasets[x], batch_size = 32, shuffle = True, num_workers = 4) for x in folders}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb44cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:33:04.745824Z",
     "iopub.status.busy": "2025-05-27T10:33:04.745269Z",
     "iopub.status.idle": "2025-05-27T10:33:04.749334Z",
     "shell.execute_reply": "2025-05-27T10:33:04.748722Z"
    },
    "papermill": {
     "duration": 0.010261,
     "end_time": "2025-05-27T10:33:04.750492",
     "exception": false,
     "start_time": "2025-05-27T10:33:04.740231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset sizes and class names\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in folders}\n",
    "class_names = image_datasets[\"train\"].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c2617",
   "metadata": {
    "papermill": {
     "duration": 0.003545,
     "end_time": "2025-05-27T10:33:04.757347",
     "exception": false,
     "start_time": "2025-05-27T10:33:04.753802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CONFIGURE & TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4255b5de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:33:04.764928Z",
     "iopub.status.busy": "2025-05-27T10:33:04.764279Z",
     "iopub.status.idle": "2025-05-27T10:33:04.889827Z",
     "shell.execute_reply": "2025-05-27T10:33:04.889002Z"
    },
    "papermill": {
     "duration": 0.130579,
     "end_time": "2025-05-27T10:33:04.891093",
     "exception": false,
     "start_time": "2025-05-27T10:33:04.760514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get torch device\n",
    "device = torch.device(\"cuda\" if torch.accelerator.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6306d426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:33:04.898662Z",
     "iopub.status.busy": "2025-05-27T10:33:04.898078Z",
     "iopub.status.idle": "2025-05-27T10:33:04.901698Z",
     "shell.execute_reply": "2025-05-27T10:33:04.901098Z"
    },
    "papermill": {
     "duration": 0.008362,
     "end_time": "2025-05-27T10:33:04.902827",
     "exception": false,
     "start_time": "2025-05-27T10:33:04.894465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make directory for saving model and stats\n",
    "os.mkdir(\"/kaggle/working/runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d573441a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:33:04.910338Z",
     "iopub.status.busy": "2025-05-27T10:33:04.909735Z",
     "iopub.status.idle": "2025-05-27T10:33:04.914301Z",
     "shell.execute_reply": "2025-05-27T10:33:04.913714Z"
    },
    "papermill": {
     "duration": 0.009224,
     "end_time": "2025-05-27T10:33:04.915347",
     "exception": false,
     "start_time": "2025-05-27T10:33:04.906123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use tensorboard to store stats and make a directory to save\n",
    "os.mkdir(\"/kaggle/working/runs/tensorboard\")\n",
    "writer = SummaryWriter(log_dir='/kaggle/working/runs/tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c274c4af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:33:04.922397Z",
     "iopub.status.busy": "2025-05-27T10:33:04.922182Z",
     "iopub.status.idle": "2025-05-27T10:33:04.930443Z",
     "shell.execute_reply": "2025-05-27T10:33:04.929847Z"
    },
    "papermill": {
     "duration": 0.013149,
     "end_time": "2025-05-27T10:33:04.931431",
     "exception": false,
     "start_time": "2025-05-27T10:33:04.918282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch: Train phase and Val phase\n",
    "            for phase in [\"train\", \"test\"]:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data\n",
    "                for inputs, labels in loader[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # Zero gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward\n",
    "                    with torch.set_grad_enabled(phase == 'train'): # grad_enable: True (Train), False (Val)\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Backward + Optimize\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Stats\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "                \n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                writer.add_scalar(f'{phase}/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar(f'{phase}/Accuracy', epoch_acc, epoch)\n",
    "\n",
    "                # Deep copy the model\n",
    "                if phase == 'test' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc5a440f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:33:04.938235Z",
     "iopub.status.busy": "2025-05-27T10:33:04.938003Z",
     "iopub.status.idle": "2025-05-27T10:33:06.457738Z",
     "shell.execute_reply": "2025-05-27T10:33:06.457058Z"
    },
    "papermill": {
     "duration": 1.524212,
     "end_time": "2025-05-27T10:33:06.458850",
     "exception": false,
     "start_time": "2025-05-27T10:33:04.934638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
      "100%|██████████| 109M/109M [00:00<00:00, 199MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ConvNeXt(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "        )\n",
       "        (3): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "        )\n",
       "        (4): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "        )\n",
       "        (5): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "        )\n",
       "        (6): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "        )\n",
       "        (7): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "        )\n",
       "        (8): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the pretrain model\n",
    "model_ft = models.convnext_tiny(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Change the classifier layer\n",
    "model_ft.classifier[2] = torch.nn.Linear(model_ft.classifier[2].in_features, len(class_names))\n",
    "\n",
    "# Check if it multigpu or not to using DataParallel (MultiGPU training)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_ft = torch.nn.DataParallel(model_ft)\n",
    "model_ft.to(device)\n",
    "\n",
    "### Output is a architecture of model after changing the out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62775df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:33:06.467695Z",
     "iopub.status.busy": "2025-05-27T10:33:06.467472Z",
     "iopub.status.idle": "2025-05-27T10:33:06.472146Z",
     "shell.execute_reply": "2025-05-27T10:33:06.471534Z"
    },
    "papermill": {
     "duration": 0.009948,
     "end_time": "2025-05-27T10:33:06.473091",
     "exception": false,
     "start_time": "2025-05-27T10:33:06.463143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross Entropy\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer - lr e-3, momentum 0.9\n",
    "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Step LR - step_size 1, gamma 0.9\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc27c132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T10:33:06.481352Z",
     "iopub.status.busy": "2025-05-27T10:33:06.481139Z",
     "iopub.status.idle": "2025-05-27T15:42:53.680216Z",
     "shell.execute_reply": "2025-05-27T15:42:53.679340Z"
    },
    "papermill": {
     "duration": 18587.204632,
     "end_time": "2025-05-27T15:42:53.681509",
     "exception": false,
     "start_time": "2025-05-27T10:33:06.476877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4358 Acc: 0.7964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.3574 Acc: 0.8385\n",
      "\n",
      "Epoch 2/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3177 Acc: 0.8624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.3145 Acc: 0.8668\n",
      "\n",
      "Epoch 3/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2892 Acc: 0.8770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2730 Acc: 0.8830\n",
      "\n",
      "Epoch 4/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2627 Acc: 0.8911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2355 Acc: 0.9025\n",
      "\n",
      "Epoch 5/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2441 Acc: 0.8987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2247 Acc: 0.9065\n",
      "\n",
      "Epoch 6/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2198 Acc: 0.9088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2283 Acc: 0.9080\n",
      "\n",
      "Epoch 7/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1931 Acc: 0.9241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2295 Acc: 0.9105\n",
      "\n",
      "Epoch 8/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1904 Acc: 0.9239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2303 Acc: 0.9065\n",
      "\n",
      "Epoch 9/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1802 Acc: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1867 Acc: 0.9250\n",
      "\n",
      "Epoch 10/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1639 Acc: 0.9334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1829 Acc: 0.9248\n",
      "\n",
      "Epoch 11/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1549 Acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1857 Acc: 0.9275\n",
      "\n",
      "Epoch 12/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1519 Acc: 0.9414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1704 Acc: 0.9297\n",
      "\n",
      "Epoch 13/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1524 Acc: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1716 Acc: 0.9300\n",
      "\n",
      "Epoch 14/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1278 Acc: 0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1875 Acc: 0.9235\n",
      "\n",
      "Epoch 15/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1268 Acc: 0.9511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1787 Acc: 0.9290\n",
      "\n",
      "Epoch 16/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1263 Acc: 0.9506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1785 Acc: 0.9307\n",
      "\n",
      "Epoch 17/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1154 Acc: 0.9559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2013 Acc: 0.9255\n",
      "\n",
      "Epoch 18/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1179 Acc: 0.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1620 Acc: 0.9377\n",
      "\n",
      "Epoch 19/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1100 Acc: 0.9581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1620 Acc: 0.9390\n",
      "\n",
      "Epoch 20/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1058 Acc: 0.9598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1655 Acc: 0.9407\n",
      "\n",
      "Epoch 21/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0985 Acc: 0.9631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1649 Acc: 0.9385\n",
      "\n",
      "Epoch 22/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0941 Acc: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1651 Acc: 0.9353\n",
      "\n",
      "Epoch 23/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0936 Acc: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1560 Acc: 0.9377\n",
      "\n",
      "Epoch 24/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0920 Acc: 0.9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1535 Acc: 0.9423\n",
      "\n",
      "Epoch 25/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0858 Acc: 0.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1525 Acc: 0.9435\n",
      "\n",
      "Training complete in 309m 47s\n",
      "Best val Acc: 0.943500\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL\n",
    "model_conv = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4637e0",
   "metadata": {
    "papermill": {
     "duration": 0.010605,
     "end_time": "2025-05-27T15:42:53.705230",
     "exception": false,
     "start_time": "2025-05-27T15:42:53.694625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SAVE & PLOT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac994a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T15:42:53.728367Z",
     "iopub.status.busy": "2025-05-27T15:42:53.728098Z",
     "iopub.status.idle": "2025-05-27T15:42:53.926709Z",
     "shell.execute_reply": "2025-05-27T15:42:53.925804Z"
    },
    "papermill": {
     "duration": 0.211947,
     "end_time": "2025-05-27T15:42:53.928325",
     "exception": false,
     "start_time": "2025-05-27T15:42:53.716378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "os.mkdir(\"/kaggle/working/runs/model\")\n",
    "torch.save(model_conv.state_dict(), \"/kaggle/working/runs/model/best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "385af1cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T15:42:53.951678Z",
     "iopub.status.busy": "2025-05-27T15:42:53.951468Z",
     "iopub.status.idle": "2025-05-27T15:44:19.861021Z",
     "shell.execute_reply": "2025-05-27T15:44:19.859993Z"
    },
    "papermill": {
     "duration": 85.922869,
     "end_time": "2025-05-27T15:44:19.862608",
     "exception": false,
     "start_time": "2025-05-27T15:42:53.939739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run model with test set\n",
    "model_conv.eval() # Eval mode\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for inputs, labels in loader[\"val\"]:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Forward - Grad disable\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model_conv(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    y_pred.append(preds)\n",
    "    y_true.append(labels)\n",
    "\n",
    "# Concatenates all the mini-batch\n",
    "y_pred = torch.cat(tuple(y_pred))\n",
    "y_true = torch.cat(tuple(y_true))\n",
    "\n",
    "# Confusion matrix\n",
    "cfs_mtx = confusion_matrix(y_true.cpu(), y_pred.cpu())\n",
    "display = ConfusionMatrixDisplay(cfs_mtx)\n",
    "\n",
    "display.plot()\n",
    "\n",
    "writer.add_figure(\"Confusion Matrix (Test set)\", display.figure_)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7475477,
     "sourceId": 11893043,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18746.075639,
   "end_time": "2025-05-27T15:44:23.774684",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-27T10:31:57.699045",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
